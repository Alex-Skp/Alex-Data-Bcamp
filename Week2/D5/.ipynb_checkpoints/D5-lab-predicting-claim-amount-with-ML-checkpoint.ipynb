{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Ironhack logo](https://i.imgur.com/1QgrNNw.png)\n",
    "\n",
    "# Lab | Predicting Claim Amount with ML Linear Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "For this lab, we still keep using the [marketing_customer_analysis.csv file](marketing_customer_analysis.csv) - the US car insurance data set. You should be able to pick up where you left off in the previous rounds of customer behaviour analysis. However this time we will look to apply a linear regression machine learning model \n",
    "\n",
    "Review the previous rounds and follow the steps as shown in previous lectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the model\n",
    "from sklearn import linear_model\n",
    "#import evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_headers(a):\n",
    "    \"\"\"\n",
    "    Input is a dataset, it will put the column labels as lower case, strip the names and subsitute spaces with dashes\n",
    "    \"\"\"\n",
    "    cols=[]\n",
    "    for i in range(len(a.columns)):\n",
    "        cols.append(a.columns[i].lower().strip().replace(\" \",\"_\"))\n",
    "        \n",
    "    a.columns = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 01 - Problem (case study)\n",
    "Familiarise yourself with Data Descriptions and the Goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### data definitions for handy reference :\n",
    "\n",
    "Unnamed: Index  \n",
    "customer: Customer ID  \n",
    "state: US State  \n",
    "customer_lifetime_value: CLV is the client economic value for a company during all their relationship  \n",
    "response: Response to marketing calls (customer engagement)  \n",
    "coverage: Customer coverage type  \n",
    "education: Customer education level  \n",
    "effective_to_date: Effective to date  \n",
    "employmentstatus: Customer employment status  \n",
    "gender: Customer gender  \n",
    "income: Customer income  \n",
    "location_code: Customer living zone  \n",
    "marital_status: Customer marital status  \n",
    "monthly_premium_auto: Monthly premium  \n",
    "months_since_last_claim: Last customer claim  \n",
    "months_since_policy_inception: Policy Inception  \n",
    "number_of_open_complaints: Open claims  \n",
    "number_of_policies: Number policies  \n",
    "policy_type: Policy type  \n",
    "policy: Policy  \n",
    "renew_offer_type: Renew  \n",
    "sales_channel: Sales channel (customer-company first contact)  \n",
    "total_claim_amount: Claims amount  \n",
    "vehicle_class: Vehicle class  \n",
    "vehicle_size: Vehicle size  \n",
    "vehicle_type: Vehicle type  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Getting Data\n",
    "Read the .csv file into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('marketing_customer_analysis.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 - Cleaning/Wrangling/EDA\n",
    "Change headers names.\n",
    "Deal with NaN values, replace with appropriate method. \n",
    "\n",
    "split categorical Features and Numerical Features.\n",
    "\n",
    "Explore visually both sets of features, to identify next steps.\n",
    "\n",
    "Look at potential multicollinearity using a correlation matrix or other approach. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Earlier we saw no NaN Values, so we keep going with cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning header names\n",
    "clean_headers(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating by categories\n",
    "y = data['total_claim_amount']\n",
    "X_num = data.select_dtypes(include = np.number)\n",
    "X_num = X_num.drop(columns = ['total_claim_amount'])\n",
    "X_cat = data.select_dtypes(include = np.object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start by exploring numerical values first\n",
    "\n",
    "X_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap to check for multicollinearity\n",
    "\n",
    "correlations_matrix = X_num.corr()\n",
    "mask = np.zeros_like(correlations_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax = sns.heatmap(correlations_matrix, mask=mask, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No high correlation, so we'll keep all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 04 - Pre-Processing Data\n",
    "Dealing with outliers.\n",
    "Normalization - ie use chosen scaler to transform selected columns into normal distribution as needed for linear regression model. Propose: MinMax scaler on 'effective_to_date' and standard scaler on numerical columns.\n",
    "\n",
    "Encoding Categorical Data fields using OHE.\n",
    "\n",
    "Bring categorical and numerical columns back together using pd.concat.\n",
    "\n",
    "Define X and y, the y value you are seeking to predict is claim amount.\n",
    "\n",
    "Splitting into train set and test dataset using random state, eg 80%:20% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with \"effective_to_date\". Have to turn it into a datetime object first, then use minmax_scale to apply to 1 column only.\n",
    "data['effective_to_date'] = pd.to_datetime(data['effective_to_date'])\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale \n",
    "dates = minmax_scale(data['effective_to_date'])\n",
    "dates = dates.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the rest of numerical columns, we standardize their distribution:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "transformer = StandardScaler().fit(X_num)\n",
    "x_standardized = transformer.transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the categorical, we will drop 'customer' as its and index for customers and tells nothing about the data, and dates\n",
    "#because we processed them earlier already\n",
    "X_cat_cl = X_cat.drop(['customer','effective_to_date'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now categorical data. We will use OneHotEncoder to codify the data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='error', drop='first').fit(X_cat_cl)\n",
    "encoded = encoder.transform(X_cat_cl).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we see if all of our X have the same size\n",
    "print(dates.shape)\n",
    "print(x_standardized.shape)\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And we put them together\n",
    "X = np.concatenate((x_standardized, encoded, dates), axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Does it match y?\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 7 split the data into train and test randomly, as a %\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 8 apply the machine learn model for It3 \n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train,y_train)\n",
    "predictions  = lm.predict(X_test)\n",
    "\n",
    "#And print the scores\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print( \"r2 Score:   \"+ str(r2))\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE Score:  \"+ str(mse))\n",
    "\n",
    "import math \n",
    "rmse = math.sqrt(mse)\n",
    "print(\"RMSE Score: \"+ str(rmse))\n",
    "\n",
    "n = len(X_test) \n",
    "p = X_test.shape[1]\n",
    "adj_r2 = 1-((1-r2)*(n-1)/(n-p-1))\n",
    "print(\"Adj. r2:    \"+str(adj_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not good enough, let's try some more stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We first define a function to clean outliers \n",
    "def clean_outliers(data,col):\n",
    "    \"\"\"\n",
    "    Will input the column name as data['Column'], will return the modified dataframe\n",
    "    \"\"\"\n",
    "    iqr = np.percentile(data[col],75) - np.percentile(data[col],25)\n",
    "    upper_limit = np.percentile(data[col],75) + 1.5*iqr\n",
    "    lower_limit = np.percentile(data[col],25) - 1.5*iqr\n",
    "\n",
    "    return data[(data[col]>lower_limit) & (data[col]<upper_limit)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And another one that rescales a column to a log scale  \n",
    "def log_rescaler(data,col):\n",
    "    \"\"\"\n",
    "    Will convert the scale of the numerical column to a log scale, and fill infinite values with the mean\n",
    "    \"\"\"\n",
    "    def log_transfom_clean_(x):\n",
    "        x = np.log(x)\n",
    "        if np.isfinite(x):\n",
    "            return x\n",
    "        else:\n",
    "            return np.NAN #  we can replace NANs with mean values in the next step   \n",
    "        \n",
    "    data[col] = list(map(log_transfom_clean_, data[col]))\n",
    "\n",
    "    return data[col].fillna(np.mean(data[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a new dframe, to keep the original untouched\n",
    "data2 = data\n",
    "#Visualizing the numeric variables \n",
    "data2.hist(figsize = (15,20));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take outliers out from monthly_premium_auto, customer_lifetime_value and income:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=data2['customer_lifetime_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean outliers, and visualize again \n",
    "\n",
    "data2 = clean_outliers(data2,'customer_lifetime_value')\n",
    "print(data2.shape)\n",
    "sns.boxplot(x=data2['customer_lifetime_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks better, now with monthly_premium_auto, and customer_lifetime_value\n",
    "data2 = clean_outliers(data2,'monthly_premium_auto')\n",
    "print(data2.shape)\n",
    "sns.boxplot(x=data2['monthly_premium_auto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = clean_outliers(data2,'income')\n",
    "print(data2.shape)\n",
    "sns.boxplot(x=data2['income'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating by categories\n",
    "y = data2['total_claim_amount']\n",
    "X_num = data2.select_dtypes(include = np.number)\n",
    "X_num = X_num.drop(columns = ['total_claim_amount'])\n",
    "X_cat = data2.select_dtypes(include = np.object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the we want to standardize distributions\n",
    "sns.distplot(X_num['customer_lifetime_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using our log_rescaler function we proceed\n",
    "X_num['customer_lifetime_value'] = log_rescaler(X_num,'customer_lifetime_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(X_num['customer_lifetime_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(X_num['monthly_premium_auto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num['monthly_premium_auto'] = log_rescaler(X_num,'monthly_premium_auto')\n",
    "sns.distplot(X_num['monthly_premium_auto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(X_num['income'])\n",
    "#we dont want to rescale this one as zero income is very significative, and we would lost this information (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standardized = X_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming dates to numerical values\n",
    "from sklearn.preprocessing import minmax_scale \n",
    "dates = minmax_scale(data2['effective_to_date'])\n",
    "dates = dates.reshape(-1,1)\n",
    "\n",
    "\n",
    "#dropping customer and date, as they are processed already\n",
    "X_cat_cl = X_cat.drop(['customer'],axis = 1)\n",
    "\n",
    "\n",
    "#Encoding categoricals with OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='error', drop='first').fit(X_cat_cl)\n",
    "encoded = encoder.transform(X_cat_cl).toarray()\n",
    "\n",
    "#Now we see if all of our X have the same size\n",
    "print(dates.shape)\n",
    "print(x_standardized.shape)\n",
    "print(encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And we put them together, we compare it with y\n",
    "X = np.concatenate((x_standardized, encoded, dates), axis=1)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 7 split the data into train and test randomly, as a %\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=120)\n",
    "\n",
    "\n",
    "#step 8 apply the machine learn model for It3 \n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train,y_train)\n",
    "predictions  = lm.predict(X_test)\n",
    "\n",
    "#And print the scores\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print( \"r2 Score:   \"+ str(r2))\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE Score:  \"+ str(mse))\n",
    "\n",
    "import math \n",
    "rmse = math.sqrt(mse)\n",
    "print(\"RMSE Score: \"+ str(rmse))\n",
    "\n",
    "n = len(X_test) \n",
    "p = X_test.shape[1]\n",
    "adj_r2 = 1-((1-r2)*(n-1)/(n-p-1))\n",
    "print(\"Adj. r2:    \"+str(adj_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 - Reporting\n",
    "Present results inside your notebook with appropriate annotation describing the accuracy of the model and business insight gained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After manually cleaning outliers and scaling the numerical values, the R2 score decreased a bit,\n",
    "#but our mean square error has been reduced, giving i believe a more accurate prediction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
