{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fantastic-lightning",
   "metadata": {},
   "source": [
    "# One step further, for a massive database of songs \n",
    "# WARNING: Big loading times zone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "configured-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from getpass import getpass\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chinese-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "c_id = str(getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rural-satellite",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "c_s = str(getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demonstrated-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=c_id,\n",
    "                                                          client_secret=c_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-given",
   "metadata": {},
   "source": [
    "We test that is running properly. We will start taking a list of categories. We test if the calls work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-ethiopia",
   "metadata": {},
   "source": [
    "And we put our categories together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "computational-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for cat in sp.categories(limit = 50)['categories']['items']:\n",
    "    categories.append(cat['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seven-imaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rough-ivory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912a48b5b4cb4b4ba5e3b580a89c34fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error for GET to https://api.spotify.com/v1/browse/categories/flamenco/playlists returned 404 due to Specified id doesn't exist\n",
      "HTTP Error for GET to https://api.spotify.com/v1/browse/categories/alternative/playlists returned 404 due to Specified id doesn't exist\n",
      "HTTP Error for GET to https://api.spotify.com/v1/browse/categories/student/playlists returned 404 due to Specified id doesn't exist\n",
      "HTTP Error for GET to https://api.spotify.com/v1/browse/categories/thirdparty/playlists returned 404 due to Specified id doesn't exist\n",
      "HTTP Error for GET to https://api.spotify.com/v1/browse/categories/salsa/playlists returned 404 due to Specified id doesn't exist\n",
      "HTTP Error for GET to https://api.spotify.com/v1/browse/categories/radar/playlists returned 404 due to Specified id doesn't exist\n",
      "HTTP Error for GET to https://api.spotify.com/v1/browse/categories/popculture/playlists returned 404 due to Specified id doesn't exist\n"
     ]
    }
   ],
   "source": [
    "playlist_id = []\n",
    "for cat in tqdm(categories):\n",
    "    try:\n",
    "        i=0\n",
    "        cat_data = sp.category_playlists(cat, limit = 50, offset=i)['playlists']['items']\n",
    "        #Having this while loop will allow retrieve more than 50 per category \n",
    "        while len(cat_data) == i+50:\n",
    "            i +=50\n",
    "            cat_data += sp.category_playlists(cat, limit = 50, offset=i)['playlists']['items']   \n",
    "    except:\n",
    "        continue\n",
    "    for plist in cat_data:\n",
    "        try:\n",
    "            playlist_id.append(plist['id'])\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affected-marathon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(playlist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "previous-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint1:saving playlist_id\n",
    "out_playlist_id = pd.Series(playlist_id, name='playlist_id')\n",
    "out_playlist_id.to_csv(\"../data/checkpoints/checkpoint_playlist_id.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-diana",
   "metadata": {},
   "source": [
    "Here we will take all artist ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "labeled-scroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f901acc13e4be581da1f020921eb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "artist_ids = []\n",
    "for plist in tqdm(playlist_id):\n",
    "    i = 0\n",
    "    tracks = sp.playlist_tracks(plist, limit=100, offset=i)['items']\n",
    "    while len(tracks)==i+100:\n",
    "        i += 100\n",
    "        tracks += sp.playlist_tracks(plist, limit=100, offset=i)['items']\n",
    "    for track in tracks:\n",
    "        try:\n",
    "            for ids in [artist['id'] for artist in track['track']['artists']]:\n",
    "                artist_ids.append(ids)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-there",
   "metadata": {},
   "source": [
    "Removing duplicated ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compressed-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24807\n"
     ]
    }
   ],
   "source": [
    "artist_ids = list(set(artist_ids))\n",
    "print(len(artist_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "seasonal-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint2:saving artist_ids\n",
    "out_artist_ids = pd.Series(artist_ids, name='artist_ids')\n",
    "out_artist_ids.to_csv(\"../data/checkpoints/checkpoint_artist_ids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-beads",
   "metadata": {},
   "source": [
    "And shuffle the list to reduce bias from the sample recovered. If it is not shuffled, the playlists from certain category would be taken first and the categories towards the end, as this script takes a lot of time to complete, won't be considered. This way we can run the script to gather as much data as we can in the period of time, and will be tracks from random artists instead of the ones from certain category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "utility-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(artist_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-bosnia",
   "metadata": {},
   "source": [
    "Now we get a list of all their albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-cleanup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5242181a52f24efa8c830a7fa102f539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "album_ids = []\n",
    "\n",
    "for artist in tqdm(artist_ids):\n",
    "        i = 0\n",
    "        albums = sp.artist_albums(artist, limit=50, offset=i)['items']\n",
    "        while len(albums)==i+50:\n",
    "            i += 50\n",
    "            albums += sp.artist_albums(artist, limit=50, offset=i)['items']\n",
    "        for album in albums:\n",
    "            album_ids.append(album['id'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-lancaster",
   "metadata": {},
   "source": [
    "Again, removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(album_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "album_ids = list(set(album_ids))\n",
    "len(album_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint2:saving artist_ids\n",
    "out_album_ids = pd.Series(album_ids, name='album_ids')\n",
    "out_album_ids.to_csv(\"../data/checkpoints/checkpoint_album_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(album_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-variable",
   "metadata": {},
   "source": [
    "Going through every album and extracting the track information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_id = []\n",
    "song_name = []\n",
    "song_artists = []\n",
    "\n",
    "\n",
    "\n",
    "for album in tqdm(album_ids):\n",
    "    try:\n",
    "        tracks = sp.album_tracks(album)['items']            \n",
    "    except:\n",
    "        continue\n",
    "    for track in tracks:\n",
    "        song_id.append(track['id'])\n",
    "        song_name.append(track['name'])\n",
    "        song_artists.append([artist['name'] for artist in track['artists']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-management",
   "metadata": {},
   "source": [
    "Creating a pandas dataframe, and dropping duplicates, if the same track was in several albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'id':song_id,\n",
    "        'name':song_name,\n",
    "        'artists':song_artists\n",
    "        }\n",
    "songs = pd.DataFrame(data=data)\n",
    "songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = songs.drop_duplicates(subset='id')\n",
    "songs.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint:saving songs\n",
    "songs.to_csv(\"../data/checkpoints/songs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst,n):\n",
    "    \"\"\"\n",
    "    Yields n-sized chunks of lst\n",
    "    \"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_ids = songs['id']\n",
    "audio_feats = []\n",
    "for chunk in tqdm(list(chunks(song_ids,50))):\n",
    "    try:\n",
    "        audio_feats.append(sp.audio_features(chunk))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-utilization",
   "metadata": {},
   "source": [
    "we flatten this list into one long list of dictionaries, and turn it into a series, in order to use pandas.dropna(, so the conversion to data frame can easily work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feats = pd.Series([y for x in audio_feats for y in x], name='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = pd.DataFrame(list(audio_feats.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint:saving features\n",
    "feat_df.to_csv(\"../data/checkpoints/feat_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = songs.merge(feat_df, on = 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-serve",
   "metadata": {},
   "source": [
    "We test that the merge worked properly using a random sample of 50, taking the audio features again, and comparing the value of danceability, for example in the resulting dataframe. The next cell will return true if there are matches, false if not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = final.sample(50).reset_index()\n",
    "(pd.DataFrame(sp.audio_features(sample['id']))['danceability'] == sample['danceability']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"../data/checkpoints/Final dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-petersburg",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-travel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
